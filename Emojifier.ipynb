{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emojifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2XRc-mdNaeq",
        "colab_type": "text"
      },
      "source": [
        "# Emojify!\n",
        "\n",
        "To build an Emojifier, we are going to use word vector representations.\n",
        "\n",
        "Have you ever wanted to make your text messages more expressive? Your emojifier app will help you do that. So rather than writing \"Congratulations on the promotion! Lets get coffee and talk. Love you!\" the emojifier can automatically turn this into \"Congratulations on the promotion! üëç Lets get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è\" Using this model, inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (‚öæÔ∏è). In many emoji interfaces, you need to remember that ‚ù§Ô∏è is the \"heart\" symbol rather than the \"love\" symbol. But using word vectors, you'll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don't even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek62-tY3jziy",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install packages and download files\n",
        "\n",
        "# Install packages\n",
        "!pip install torch torchvision -q\n",
        "!pip install emoji -q\n",
        "# Download 50 dimentional GLoVe embedding\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!rm *.*.*00d.txt\n",
        "!rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR9VUNWOkBWv",
        "colab_type": "code",
        "outputId": "86cf2205-9a37-47a7-d649-fcdfb6a5f6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import csv\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfHBl9hCkMvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To read glove vector embedding\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByfdwUH6kPZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
        "                    \"1\": \":baseball:\",\n",
        "                    \"2\": \":smile:\",\n",
        "                    \"3\": \":disappointed:\",\n",
        "                    \"4\": \":fork_and_knife:\"}\n",
        "\n",
        "def label_to_emoji(label):\n",
        "    \"\"\"\n",
        "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
        "    \"\"\"\n",
        "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyVJl9IkkVo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_csv(filename):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open(filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0].strip())\n",
        "            emoji.append(row[1].strip())\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBq3nG-xLMOQ",
        "colab_type": "text"
      },
      "source": [
        "Load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbqu5WPlkYyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train = read_csv('train_emoji.csv')\n",
        "X_test, Y_test = read_csv('tesss.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YOUosIEkbnv",
        "colab_type": "code",
        "outputId": "a1640e34-242d-47dc-a833-8278ddf85b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Training set size:', len(X_train))\n",
        "print('Test set size:', len(X_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size: 132\n",
            "Test set size: 56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC5iYfZcked6",
        "colab_type": "code",
        "outputId": "e4d42069-b2ab-4936-8f65-40dae6e6b88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "# First 10 rows\n",
        "for i in range(10):\n",
        "    print(X_train[i], label_to_emoji(Y_train[i]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "never talk to me again üòû\n",
            "I am proud of your achievements üòÑ\n",
            "It is the worst day in my life üòû\n",
            "Miss you so much ‚ù§Ô∏è\n",
            "food is life üç¥\n",
            "I love you mum ‚ù§Ô∏è\n",
            "Stop saying bullshit üòû\n",
            "congratulations on your acceptance üòÑ\n",
            "The assignment is too long üòû\n",
            "I want to go play ‚öæ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTkpXAUukh56",
        "colab_type": "code",
        "outputId": "933b84b0-505e-48b0-9852-ace974ad62ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())\n",
        "maxLen"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTIGugxOkkWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the GLoVe embedding\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy0qOGraME01",
        "colab_type": "text"
      },
      "source": [
        "* `word_to_index`: dictionary mapping from words to their indices in the vocabulary (400,001 words, with the valid indices ranging from 0 to 400,000)\n",
        "\n",
        "* `index_to_word`: dictionary mapping from indices to their corresponding words in the vocabulary\n",
        "\n",
        "* `word_to_vec_map`: dictionary mapping words to their GloVe vector representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFNvVzCVksQR",
        "colab_type": "code",
        "outputId": "68d90da0-99d7-45b2-de04-6fa4c09f73eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# as a example\n",
        "word = \"india\"\n",
        "index = 289846\n",
        "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
        "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the index of india in the vocabulary is 189129\n",
            "the 289846th word in the vocabulary is potatos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJD8WOnrMMzc",
        "colab_type": "text"
      },
      "source": [
        "Convert all your training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence.\n",
        "\n",
        "This function below to convert X (array of sentences as strings) into an array of indices corresponding to words in the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBogNuEwkz6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()`. \n",
        "    \n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "    \n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]                                   # number of training examples\n",
        "    \n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (‚âà 1 line)\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "    \n",
        "    for i in range(m):                               # loop over training examples\n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words = X[i].lower().split()\n",
        "        \n",
        "        # Initialize j to -1\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        # For Left zero padding\n",
        "        for w in reversed(sentence_words): \n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            X_indices[i, j-1] = word_to_index[w]\n",
        "            # decrement j to j - 1\n",
        "            j = j - 1\n",
        "            \n",
        "        # For Right zero padding\n",
        "        # for w in sentence_words: \n",
        "        #    # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "        #    X_indices[i, j] = word_to_index[w]\n",
        "        #    # decrement j to j + 1\n",
        "        #    j = j + 1\n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbxJ6d3mk33D",
        "colab_type": "code",
        "outputId": "a4fe6d80-79f9-472b-a28c-886efd6f240b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "# example\n",
        "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
        "X1_indices = sentences_to_indices(X1,word_to_index, max_len=5)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"\\nX1_indices =\", X1_indices)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
            "\n",
            "X1_indices = [[     0.      0.      0. 155345. 225122.]\n",
            " [     0.      0. 220930. 286375.  69714.]\n",
            " [151204. 192973. 302254. 151349. 394475.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwzNfCFFl77j",
        "colab_type": "code",
        "outputId": "c011d471-cc77-476b-9b03-b1fb6f8c3c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# train and test shape\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((132,), (56,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8EEtLwDl_Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "X_te = sentences_to_indices(X_test, word_to_index, maxLen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ea-hK9EmCjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(Y_train))\n",
        "test_data = TensorDataset(torch.from_numpy(X_te), torch.from_numpy(Y_test))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 4\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27jXqUWEmML5",
        "colab_type": "code",
        "outputId": "7df06814-54ea-435f-bcd7-9b707b27713b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# input shape in the network\n",
        "x, y = next(iter(train_loader))\n",
        "x.shape # shape = (batch_size, maxLen)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQUWZvTqmPPf",
        "colab_type": "code",
        "outputId": "d9a0692c-4a25-4df2-f596-8ce664cffcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# just checking\n",
        "for i in range(x.size(0)):\n",
        "    print(' '.join([index_to_word[int(xx)] for xx in x[i] if xx != 0]), label_to_emoji(int(y[i])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he likes baseball ‚öæ\n",
            "i cooked meat üç¥\n",
            "she is so cute ‚ù§Ô∏è\n",
            "where is the stadium ‚öæ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioRkKtVVmS04",
        "colab_type": "code",
        "outputId": "8b3c86f8-be68-4df8-feda-7a1589ff6530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# First checking if GPU is available\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "print('Training on GPU.') if train_on_gpu else print('No GPU available, training on CPU.')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPmBYyFSmV5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]    \n",
        "    \n",
        "    weight = torch.FloatTensor(emb_matrix)\n",
        "    embedding_layer = nn.Embedding.from_pretrained(weight.cuda())\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBf5h3fzmaSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Emojify(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to find correct emoji.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size, hidden_dim, n_layers, word_to_vec_map, word_to_index, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(Emojify, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # define all layers\n",
        "\n",
        "        self.embedding = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "        embedding_dim = self.embedding.embedding_dim # embedding dimension\n",
        "    \n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and logsoftmax layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        x = x.type(torch.cuda.LongTensor)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        \n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden) # lstm_out.shape = (4, 10, 128)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # logsoftmax function\n",
        "        logsoftmax_out = self.logsoftmax(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        logsoftmax_out = logsoftmax_out.view(batch_size, -1)\n",
        "        \n",
        "        # return last logsoftmax output and hidden state\n",
        "        return logsoftmax_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = torch.tensor((),) \n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2evwX1u-miEb",
        "colab_type": "code",
        "outputId": "822b5560-a48e-4a9d-8f3c-fbbd66c0b1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "output_size = 5\n",
        "hidden_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "net = Emojify(output_size, hidden_dim, n_layers, word_to_vec_map, word_to_index, drop_prob=0.5)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emojify(\n",
            "  (embedding): Embedding(400001, 50)\n",
            "  (lstm): LSTM(50, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3)\n",
            "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
            "  (logsoftmax): LogSoftmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUpMnkVxml8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=0.004\n",
        "\n",
        "criterion = nn.NLLLoss() # or use nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhUqAv4BmptJ",
        "colab_type": "code",
        "outputId": "e33db045-9235-40f9-9e03-cd20a4c359c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "print_every = 5\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if train_on_gpu:\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        labels = labels.type(torch.cuda.LongTensor)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "    \n",
        "    if e%print_every == print_every-1:\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "              \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Loss: 1.566547...\n",
            "Epoch: 10/100... Loss: 1.686284...\n",
            "Epoch: 15/100... Loss: 1.186079...\n",
            "Epoch: 20/100... Loss: 0.678496...\n",
            "Epoch: 25/100... Loss: 1.100852...\n",
            "Epoch: 30/100... Loss: 0.401501...\n",
            "Epoch: 35/100... Loss: 0.145676...\n",
            "Epoch: 40/100... Loss: 0.060932...\n",
            "Epoch: 45/100... Loss: 0.026466...\n",
            "Epoch: 50/100... Loss: 0.145202...\n",
            "Epoch: 55/100... Loss: 0.712100...\n",
            "Epoch: 60/100... Loss: 0.582608...\n",
            "Epoch: 65/100... Loss: 0.260361...\n",
            "Epoch: 70/100... Loss: 0.036270...\n",
            "Epoch: 75/100... Loss: 0.009214...\n",
            "Epoch: 80/100... Loss: 0.003649...\n",
            "Epoch: 85/100... Loss: 0.001581...\n",
            "Epoch: 90/100... Loss: 0.261765...\n",
            "Epoch: 95/100... Loss: 0.011587...\n",
            "Epoch: 100/100... Loss: 0.009255...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyU0Yg5FmzTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_and_prediction(net, loader=test_loader, batch_size=batch_size):\n",
        "    net.eval()\n",
        "    accuracy = 0\n",
        "    loss = 0\n",
        "    classes = []\n",
        "    val_h = net.init_hidden(batch_size)\n",
        "    for inputs, labels in loader:\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        labels = labels.type(torch.cuda.LongTensor)\n",
        "\n",
        "        log_ps, val_h = net(inputs, val_h)\n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        \n",
        "        accuracy += torch.mean(equals.type(torch.cuda.FloatTensor))\n",
        "        loss += criterion(log_ps, labels).item()\n",
        "\n",
        "        for i in top_class.squeeze():\n",
        "            classes.append(int(i))\n",
        "\n",
        "    net.train()\n",
        "    loss, accuracy = float(loss/len(loader)), float(accuracy/len(loader))\n",
        "    \n",
        "    return loss, accuracy, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jImltQUxUIta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "360093c9-1ae4-426e-c872-177152672182"
      },
      "source": [
        "_, train_accuracy, _ = evaluate_and_prediction(net, train_loader, batch_size)\n",
        "\n",
        "print('Train Accuracy: {:.3f}'.format(train_accuracy))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGCgaDthm2Tr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cc6b35be-e6e0-44fc-b637-af5ef203bbde"
      },
      "source": [
        "test_loss, test_accuracy, predicted = evaluate_and_prediction(net, test_loader, batch_size)\n",
        "\n",
        "print('Test Loss: {:.3f}'.format(test_loss),\n",
        "     '\\nTest Accuracy: {:.3f}'.format(test_accuracy))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.619 \n",
            "Test Accuracy: 0.821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lro6v6Rm9sO",
        "colab_type": "code",
        "outputId": "52394f17-87f4-442d-d592-849fa4674b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "# check\n",
        "for i in range(10):\n",
        "    print(X_test[i], '\\nAcctualy:', label_to_emoji(Y_test[i]), \\\n",
        "          'Predicted:', label_to_emoji(predicted[i]), end='\\n\\n')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I want to eat \n",
            "Acctualy: üç¥ Predicted: üç¥\n",
            "\n",
            "he did not answer \n",
            "Acctualy: üòû Predicted: üòû\n",
            "\n",
            "he got a very nice raise \n",
            "Acctualy: üòÑ Predicted: ‚ù§Ô∏è\n",
            "\n",
            "she got me a nice present \n",
            "Acctualy: üòÑ Predicted: ‚ù§Ô∏è\n",
            "\n",
            "ha ha ha it was so funny \n",
            "Acctualy: üòÑ Predicted: üòÑ\n",
            "\n",
            "he is a good friend \n",
            "Acctualy: üòÑ Predicted: ‚ù§Ô∏è\n",
            "\n",
            "I am upset \n",
            "Acctualy: üòû Predicted: üòû\n",
            "\n",
            "We had such a lovely dinner tonight \n",
            "Acctualy: üòÑ Predicted: üòÑ\n",
            "\n",
            "where is the food \n",
            "Acctualy: üç¥ Predicted: üç¥\n",
            "\n",
            "Stop making this joke ha ha ha \n",
            "Acctualy: üòÑ Predicted: üòÑ\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}